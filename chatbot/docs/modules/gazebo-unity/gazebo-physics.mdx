---
title: "Gazebo Physics Simulation"
sidebar_position: 1
description: "Understanding Gazebo physics simulation for humanoid robotics including gravity, collision handling, and sensor simulation"
---

# Gazebo Physics Simulation for Humanoid Robotics

## Introduction

Gazebo is a powerful physics-based simulation environment that provides realistic simulation capabilities for robotics applications. For humanoid robotics, Gazebo offers sophisticated physics modeling that accurately represents real-world interactions, making it an essential tool for testing and validating humanoid robot behaviors before deployment on physical hardware.

This chapter covers the core concepts of Gazebo physics simulation specifically tailored for humanoid robotics applications, including gravity simulation, collision handling, and sensor integration.

## Chapter Overview

This chapter is structured to provide a comprehensive understanding of Gazebo physics simulation for humanoid robots:

1. **Core Physics Concepts** - Understanding the fundamental physics principles that govern simulation behavior
2. **Environment Modeling** - Creating appropriate environments for humanoid robot testing
3. **Sensor Simulation** - Configuring and using various sensors within the simulation
4. **Launch Configuration** - Setting up and running Gazebo simulations with ROS 2
5. **Best Practices** - Guidelines for effective and efficient simulation development

By the end of this chapter, you will understand how to create, configure, and run physics-based simulations suitable for humanoid robotics development and testing.

## Core Physics Concepts

### Physics Engine Fundamentals

Gazebo utilizes advanced physics engines to simulate realistic physical interactions. The primary physics engines supported are:

- **ODE (Open Dynamics Engine)**: Default engine, good balance of performance and accuracy
- **Bullet**: Offers advanced collision detection and response
- **DART (Dynamic Animation and Robotics Toolkit)**: Specialized for humanoid robots with advanced constraint handling

The physics engine is responsible for:

- **Gravity Simulation**: Accurately modeling gravitational forces that affect all objects in the simulation
- **Collision Detection**: Identifying when objects make contact with each other
- **Collision Response**: Calculating the resulting forces and motions when collisions occur
- **Friction Modeling**: Simulating surface interactions and resistance to motion
- **Joint Constraints**: Maintaining proper kinematic relationships between robot links

Each physics engine has its strengths and trade-offs. For humanoid robotics, ODE is often the default choice due to its stability and widespread use in ROS ecosystem.

### Gravity Configuration

Gravity in Gazebo is configured globally for the entire simulation environment. The default gravity vector is typically set to Earth's gravity (9.81 m/sÂ²) in the negative Z direction, but this can be customized for different scenarios:

```xml
<!-- In world file -->
<sdf version='1.7'>
  <world name='default'>
    <physics type='ode'>
      <gravity>0 0 -9.8</gravity>
      <max_step_size>0.001</max_step_size>
      <real_time_factor>1</real_time_factor>
      <real_time_update_rate>1000</real_time_update_rate>
    </physics>
    <!-- Additional world configuration -->
  </world>
</sdf>
```

For humanoid robotics, the gravity configuration is crucial as it affects:
- Walking and balance algorithms
- Manipulation tasks
- Overall robot stability and behavior

#### Custom Gravity Scenarios

You can configure different gravity scenarios for testing:

```xml
<!-- Moon gravity (1/6 of Earth) -->
<gravity>0 0 -1.63</gravity>

<!-- Zero gravity (for space robotics) -->
<gravity>0 0 0</gravity>

<!-- Custom direction (for tilted environments) -->
<gravity>0.5 0 -9.3</gravity>
```

#### Physics Engine Parameters

The physics engine parameters significantly impact simulation accuracy and performance:

- **max_step_size**: The maximum time step size for the physics engine (typically 0.001s)
- **real_time_factor**: Target speed of simulation relative to real time (1.0 = real-time)
- **real_time_update_rate**: Update rate for physics calculations (Hz)

For humanoid robotics, it's recommended to use smaller time steps to ensure stable joint control and accurate contact modeling.

### Collision Handling for Humanoids

Humanoid robots require special attention to collision handling due to their complex kinematic structure and the need for safe interactions. Key considerations include:

- **Self-Collision Avoidance**: Preventing robot links from colliding with each other during movement
- **Environment Collision**: Detecting and responding to collisions with the environment
- **Contact Stabilization**: Ensuring stable contact between robot feet and ground surface
- **Friction Parameters**: Setting appropriate friction coefficients for different surfaces

#### Self-Collision Configuration

For humanoid robots, self-collision detection is typically disabled for kinematically connected links to improve performance, as these collisions are expected and planned. However, it's important to maintain collision detection for:

- Links that can potentially collide during unusual movements
- End effectors that might contact other parts of the robot
- Links with complex kinematic constraints

In URDF/SDF models, you can specify which collision pairs to ignore:

```xml
<!-- In URDF, using the 'self_collide' property in joint definitions -->
<joint name="shoulder_to_elbow" type="revolute">
  <parent link="upper_arm"/>
  <child link="lower_arm"/>
  <safety_controller self_collide="false"/>
</joint>
```

#### Contact Stabilization for Bipedal Locomotion

For humanoid robots walking in Gazebo, contact stabilization is critical for realistic walking behavior:

```xml
<!-- In ground plane collision properties -->
<collision name='collision'>
  <surface>
    <friction>
      <ode>
        <mu>1.0</mu>  <!-- Static friction coefficient -->
        <mu2>1.0</mu2>  <!-- Secondary friction coefficient -->
        <slip1>0.0</slip1>  <!-- Primary slip coefficient -->
        <slip2>0.0</slip2>  <!-- Secondary slip coefficient -->
      </ode>
    </friction>
    <contact>
      <ode>
        <soft_cfm>0.001</soft_cfm>  <!-- Constraint Force Mixing -->
        <soft_erp>0.2</soft_erp>  <!-- Error Reduction Parameter -->
        <kp>1e+10</kp>  <!-- Spring stiffness -->
        <kd>1</kd>  <!-- Damping coefficient -->
      </ode>
    </contact>
  </surface>
</collision>
```

#### Environment Modeling for Humanoid Interaction

When creating environments for humanoid robots, consider these factors:

**Floor Surfaces**:
- Use appropriate friction coefficients (typically 0.5-1.0 for walking surfaces)
- Ensure sufficient collision geometry resolution for stable foot contact
- Consider different surface types (carpet, tile, wood) with varying properties

**Obstacle Navigation**:
- Place obstacles at appropriate heights for humanoid-sized robot
- Ensure doorways and passages are wide enough for humanoid navigation
- Include interactive objects (tables, chairs) at appropriate heights

**Safety Boundaries**:
- Implement virtual boundaries to keep robots within safe operating areas
- Use collision geometry to prevent robots from falling off platforms
- Consider adding safety zones around the robot workspace

#### Collision Geometry Optimization

For humanoid robots, balancing collision accuracy with performance is crucial:

- **Simplified Collision Models**: Use simpler geometric shapes (boxes, cylinders) instead of complex meshes for collision detection
- **Multi-resolution Collision**: Use detailed collision models only when necessary
- **Collision Filtering**: Properly configure which objects should collide with each other

Example of optimized collision geometry for a humanoid limb:

```xml
<link name="upper_arm">
  <collision>
    <!-- Simplified collision shape -->
    <geometry>
      <cylinder radius="0.05" length="0.3"/>
    </geometry>
    <origin xyz="0 0 0.15" rpy="0 0 0"/>
  </collision>
</link>
```

## Environment Modeling for Humanoids

### Creating Humanoid-Friendly Environments

When designing simulation environments for humanoid robots, consider the following:

- **Floor Surfaces**: Ensure proper friction and restitution values for stable walking
- **Obstacle Placement**: Position obstacles at appropriate heights for humanoid navigation
- **Interaction Objects**: Include objects that the humanoid can manipulate or interact with
- **Safety Boundaries**: Implement virtual boundaries to keep the robot within safe operating areas

### Example Environment Configuration

```xml
<sdf version='1.7'>
  <world name='humanoid_lab'>
    <!-- Physics parameters -->
    <physics type='ode'>
      <max_step_size>0.001</max_step_size>
      <real_time_factor>1</real_time_factor>
      <real_time_update_rate>1000</real_time_update_rate>
    </physics>

    <!-- Gravity -->
    <gravity>0 0 -9.8</gravity>

    <!-- Ground plane -->
    <include>
      <uri>model://ground_plane</uri>
    </include>

    <!-- Lighting -->
    <include>
      <uri>model://sun</uri>
    </include>

    <!-- Optional: Add specific environment elements -->
    <!-- Tables, chairs, doors, etc. for humanoid interaction -->
  </world>
</sdf>
```

## Sensor Simulation in Gazebo

### LiDAR Sensors

LiDAR (Light Detection and Ranging) sensors are crucial for humanoid navigation and mapping. In Gazebo, LiDAR sensors can be configured with various parameters:

```xml
<sensor name='humanoid_laser' type='ray'>
  <pose>0.1 0 0.1 0 0 0</pose>
  <ray>
    <scan>
      <horizontal>
        <samples>720</samples>
        <resolution>1</resolution>
        <min_angle>-1.570796</min_angle>  <!-- -90 degrees -->
        <max_angle>1.570796</max_angle>   <!-- 90 degrees -->
      </horizontal>
    </scan>
    <range>
      <min>0.1</min>
      <max>30.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <plugin name='laser_controller' filename='libgazebo_ros_ray_sensor.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/out:=scan</remapping>
    </ros>
    <output_type>sensor_msgs/LaserScan</output_type>
  </plugin>
</sensor>
```

### Depth Camera Sensors

Depth cameras provide both visual and depth information, essential for humanoid perception tasks:

```xml
<sensor name='humanoid_depth_camera' type='depth'>
  <pose>0 0 0.2 0 0 0</pose>
  <camera>
    <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>10</far>
    </clip>
  </camera>
  <plugin name='camera_controller' filename='libgazebo_ros_openni_kinect.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/rgb/image_raw:=camera/color/image_raw</remapping>
      <remapping>~/depth/image_raw:=camera/depth/image_raw</remapping>
    </ros>
    <update_rate>30.0</update_rate>
  </plugin>
</sensor>
```

### IMU Sensors

Inertial Measurement Unit (IMU) sensors are critical for humanoid balance and orientation:

```xml
<sensor name='humanoid_imu' type='imu'>
  <pose>0 0 0.8 0 0 0</pose>  <!-- Positioned at torso level -->
  <imu>
    <angular_velocity>
      <x>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </x>
      <y>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </y>
      <z>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </z>
    </angular_velocity>
    <linear_acceleration>
      <x>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </x>
      <y>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </y>
      <z>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </z>
    </linear_acceleration>
  </imu>
  <plugin name='imu_controller' filename='libgazebo_ros_imu.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/out:=imu/data</remapping>
    </ros>
  </plugin>
</sensor>
```

## Gazebo Launch Files

### Creating Launch Configurations

Gazebo simulations are typically launched using ROS 2 launch files that coordinate the physics simulation with ROS 2 nodes:

```python
# examples/gazebo-unity/gazebo/launch/humanoid_sim.launch.py
import os
from launch import LaunchDescription
from launch.actions import IncludeLaunchDescription
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import PathJoinSubstitution
from launch_ros.actions import Node
from launch_ros.substitutions import FindPackageShare

def generate_launch_description():
    pkg_gazebo_ros = FindPackageShare('gazebo_ros')

    # Gazebo launch
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            PathJoinSubstitution([pkg_gazebo_ros, 'launch', 'gazebo.launch.py'])
        ),
        launch_arguments={
            'world': PathJoinSubstitution([FindPackageShare('your_robot_description'), 'worlds', 'humanoid_lab.sdf']),
            'verbose': 'true'
        }.items()
    )

    return LaunchDescription([
        gazebo,
    ])
```

## Sensor Simulation in Gazebo

### Overview of Sensor Types for Humanoid Robots

Sensors are critical components for humanoid robot perception and control in simulation environments. Gazebo provides realistic simulation of various sensor types that are commonly used in humanoid robotics:

- **LiDAR Sensors**: For navigation, mapping, and obstacle detection
- **Depth Cameras**: For 3D perception, object recognition, and scene understanding
- **IMU Sensors**: For orientation, balance, and motion detection
- **Force/Torque Sensors**: For contact detection and manipulation
- **Camera Sensors**: For visual perception and recognition tasks

Each sensor type has specific configuration parameters that affect both the realism of the simulation and computational performance.

### LiDAR Sensor Simulation

LiDAR (Light Detection and Ranging) sensors are essential for humanoid navigation and mapping. In Gazebo, LiDAR sensors simulate the emission of laser beams and detection of reflections to measure distances to objects.

#### Configuration Parameters

Key parameters for LiDAR configuration include:

- **Samples**: Number of rays in the horizontal scan (affects resolution)
- **Range**: Minimum and maximum detection distance
- **Field of View**: Angular coverage of the sensor
- **Update Rate**: How frequently the sensor data is refreshed

#### Example Configuration

```xml
<sensor name='humanoid_laser' type='ray'>
  <pose>0.1 0 0.1 0 0 0</pose>
  <ray>
    <scan>
      <horizontal>
        <samples>720</samples>
        <resolution>1</resolution>
        <min_angle>-1.570796</min_angle>  <!-- -90 degrees -->
        <max_angle>1.570796</max_angle>   <!-- 90 degrees -->
      </horizontal>
    </scan>
    <range>
      <min>0.1</min>
      <max>30.0</max>
      <resolution>0.01</resolution>
    </range>
  </ray>
  <plugin name='laser_controller' filename='libgazebo_ros_ray_sensor.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/out:=scan</remapping>
    </ros>
    <output_type>sensor_msgs/LaserScan</output_type>
  </plugin>
</sensor>
```

### Depth Camera Simulation

Depth cameras provide both visual and depth information, which is essential for humanoid perception tasks. They simulate RGB-D sensors like the Microsoft Kinect or Intel RealSense.

#### Key Features

- **Color Information**: RGB image data for visual processing
- **Depth Data**: Distance measurements for 3D reconstruction
- **Point Clouds**: 3D point cloud generation from depth data

#### Configuration Example

```xml
<sensor name='humanoid_depth_camera' type='depth'>
  <pose>0 0 0.2 0 0 0</pose>
  <camera>
    <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->
    <image>
      <width>640</width>
      <height>480</height>
      <format>R8G8B8</format>
    </image>
    <clip>
      <near>0.1</near>
      <far>10</far>
    </clip>
  </camera>
  <plugin name='camera_controller' filename='libgazebo_ros_openni_kinect.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/rgb/image_raw:=camera/color/image_raw</remapping>
      <remapping>~/depth/image_raw:=camera/depth/image_raw</remapping>
    </ros>
    <update_rate>30.0</update_rate>
  </plugin>
</sensor>
```

### IMU Sensor Simulation

Inertial Measurement Unit (IMU) sensors are critical for humanoid balance and orientation. They provide measurements of linear acceleration, angular velocity, and sometimes magnetic field.

#### Importance for Humanoid Robots

- **Balance Control**: Essential for bipedal locomotion
- **Orientation Estimation**: Critical for navigation and manipulation
- **Motion Detection**: Helps detect falls or unexpected movements

#### Configuration with Noise Modeling

Real IMUs have noise characteristics that should be simulated for realistic performance:

```xml
<sensor name='humanoid_imu' type='imu'>
  <pose>0 0 0.8 0 0 0</pose>  <!-- Positioned at torso level -->
  <imu>
    <angular_velocity>
      <x>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </x>
      <y>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </y>
      <z>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>2e-4</stddev>
        </noise>
      </z>
    </angular_velocity>
    <linear_acceleration>
      <x>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </x>
      <y>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </y>
      <z>
        <noise type='gaussian'>
          <mean>0.0</mean>
          <stddev>1.7e-2</stddev>
        </noise>
      </z>
    </linear_acceleration>
  </imu>
  <plugin name='imu_controller' filename='libgazebo_ros_imu.so'>
    <ros>
      <namespace>/humanoid</namespace>
      <remapping>~/out:=imu/data</remapping>
    </ros>
  </plugin>
</sensor>
```

### Sensor Integration with ROS 2

All Gazebo sensors can be integrated with ROS 2 through appropriate plugins that publish sensor data to ROS topics following standard message types:

- **LaserScan**: `sensor_msgs/msg/LaserScan`
- **Image**: `sensor_msgs/msg/Image`
- **CameraInfo**: `sensor_msgs/msg/CameraInfo`
- **PointCloud2**: `sensor_msgs/msg/PointCloud2`
- **Imu**: `sensor_msgs/msg/Imu`
- **JointState**: `sensor_msgs/msg/JointState`

This integration allows the same algorithms to run in simulation and on real robots with minimal changes.

### Launch Files and Plugin Configuration

Gazebo simulations are typically orchestrated using ROS 2 launch files that coordinate the physics simulation with sensor plugins and robot controllers. Here's an example of a comprehensive launch file for a humanoid simulation:

```python
# examples/gazebo-unity/gazebo/launch/humanoid_sim.launch.py
import os
from ament_index_python.packages import get_package_share_directory
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, TimerAction
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
from launch_ros.actions import Node, ComposableNodeContainer
from launch_ros.descriptions import ComposableNode

def generate_launch_description():
    # Package locations
    pkg_gazebo_ros = get_package_share_directory('gazebo_ros')
    pkg_robot_description = get_package_share_directory('robot_description')

    # Launch arguments
    use_sim_time = LaunchConfiguration('use_sim_time', default='true')
    world = LaunchConfiguration('world', default='humanoid_lab.sdf')

    # Gazebo launch
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(pkg_gazebo_ros, 'launch', 'gazebo.launch.py')
        ),
        launch_arguments={
            'world': PathJoinSubstitution([get_package_share_directory('robot_description'), 'worlds', world]),
            'verbose': 'false',
            'gui': 'true'
        }.items()
    )

    # Robot state publisher
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        name='robot_state_publisher',
        output='screen',
        parameters=[{
            'use_sim_time': use_sim_time,
            'robot_description': open(os.path.join(pkg_robot_description, 'urdf', 'humanoid.urdf')).read()
        }]
    )

    # Joint state publisher
    joint_state_publisher = Node(
        package='joint_state_publisher',
        executable='joint_state_publisher',
        name='joint_state_publisher',
        parameters=[{'use_sim_time': use_sim_time}]
    )

    return LaunchDescription([
        DeclareLaunchArgument('world', default_value='humanoid_lab.sdf', description='Choose one of the world files from `/robot_description/worlds`'),
        gazebo,
        robot_state_publisher,
        joint_state_publisher,
    ])
```

#### Plugin Configuration Best Practices

When configuring plugins for humanoid simulation:

1. **Namespace Organization**: Use consistent namespaces for all robot topics
2. **Resource Management**: Configure appropriate update rates to balance accuracy and performance
3. **Error Handling**: Implement proper error handling in plugin configurations
4. **Topic Remapping**: Use clear topic names that match your robot's configuration

## Best Practices for Humanoid Simulation

### Performance Optimization

- **Simplify Collision Models**: Use simpler collision geometries for faster computation
- **Adjust Physics Parameters**: Balance accuracy with performance requirements
- **Limit Sensor Update Rates**: Set appropriate update rates based on application needs
- **Use Efficient World Models**: Optimize environment complexity for simulation speed

### Accuracy Considerations

- **Validate Against Real Data**: Compare simulation results with real robot behavior
- **Tune Physical Parameters**: Adjust friction, restitution, and other parameters for realism
- **Sensor Noise Modeling**: Include realistic noise models for sensor data
- **Calibrate Joint Dynamics**: Ensure joint friction and damping match real hardware

## Summary

Gazebo provides a comprehensive physics simulation environment ideal for humanoid robotics development. By properly configuring gravity, collision handling, and sensor simulation, developers can create realistic simulation environments that closely match real-world behavior. This enables safe testing and validation of humanoid robot algorithms before deployment on physical hardware.

The next section will cover Unity integration for high-fidelity visualization of these physics simulations.